{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f48d24",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2d844",
   "metadata": {},
   "source": [
    "- A supervised machine learning classification tool\n",
    "- Regression (Regression trees)\n",
    "\n",
    "<img  src=\"./images/Wk7_1.png\"  style=\" width:80%; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c2ad1",
   "metadata": {},
   "source": [
    "### 1. Learning Process (Decision Tree Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8227ed",
   "metadata": {},
   "source": [
    "1. Find training set\n",
    "2. Decide what feature to ue at root node\n",
    "3. Decide left branch node/feature to split on\n",
    "4. Decide right branch node/feature to split on\n",
    "5. Stop when completely classified and no mix of labels/maximum purity\n",
    "\n",
    "Decisions:\n",
    "1. How to **choose what feature to split** on at each node?\n",
    "2. **When do you stop splitting?**\n",
    "    * When a node is 100% one class\n",
    "    * When splitting a node will result in tree exceeding a maximum depth (prevent ovefitting/complex model)\n",
    "    * When improvements in purity score are below a threshold\n",
    "    * When number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750cc2d",
   "metadata": {},
   "source": [
    "### 2. Measuring Purity using Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3a918",
   "metadata": {},
   "source": [
    "**Entropy** - Measure of impurity of a set of data\n",
    "\n",
    "\n",
    "Let $p_1$ = fraction of examples that are cats\n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "\n",
    "$H(p_1) = 1$ is highest (most impure) when $p_1 =  0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d493e",
   "metadata": {},
   "source": [
    "### 3. Choosing a split: Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a30b1",
   "metadata": {},
   "source": [
    "- Choose feature to split on such that it the lowest average weightage of entropy ($H(p_1)$)  \n",
    "$w$ = number of data points in node/ total from parent node\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5a3ca",
   "metadata": {},
   "source": [
    "### 4. Building a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb652c9",
   "metadata": {},
   "source": [
    "1. Start with all examples at **root node**.\n",
    "2. Calculate information gain for all possible features, pick one with **highest information gain**\n",
    "3. **Split dataset** according to selected feature, and **create left and right branches**\n",
    "4. **Repeat** splitting process **until one of stopping criteria** is met (See above).\n",
    "5. When stop splitting, turn into **leaf node**, i.e. a **label** to the node\n",
    "\n",
    "<img  src=\"./images/Wk7_2.png\"  style=\" width:80%; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ba7d4",
   "metadata": {},
   "source": [
    "### 5. One-hot encoding of categorical features\n",
    "- For more than one categorical values for each feature\n",
    "- Eg: Ear shape (Pointy, Oval, Floppy)\n",
    "\n",
    "\n",
    "**One-hot encoding**: If a categorical feature can take on $k$ values, create $k$ binary features (0 or 1 valued)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34af27",
   "metadata": {},
   "source": [
    "### 6. Continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdff57",
   "metadata": {},
   "source": [
    "- Splitting on continuous variable\n",
    "     * Split on threshold which provides most information gain by trying different values to split on\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0954130",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c4fa4",
   "metadata": {},
   "source": [
    "### 1. Regression with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a410961",
   "metadata": {},
   "source": [
    "Example dataset:\n",
    "<img  src=\"./images/Wk7_3.png\"  style=\" width:70%; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eef272",
   "metadata": {},
   "source": [
    "### 2. Choosing a Split\n",
    "- Rather than reducing entropy, we want to reduce **VARIANCE of values y in each subsets of data**.\n",
    "\n",
    "\n",
    "Information gain: reduction in variance\n",
    "$$\\text{Information Gain} = V(p_1^\\text{node})- (w^{\\text{left}}V(p_1^\\text{left}) + w^{\\text{right}}V(p_1^\\text{right}))$$\n",
    "<img  src=\"./images/Wk7_4.png\"  style=\" width:70%; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d253118",
   "metadata": {},
   "source": [
    "## Tree ensembles (Multiple decision trees)\n",
    "-  Bagged Decision Tree, Random Forest Algorithm, XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1039a7d",
   "metadata": {},
   "source": [
    "### 1. Why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca456f9",
   "metadata": {},
   "source": [
    "- Trees are highly sensitive to small changes of data\n",
    "- Changing one example might cause totally different tree to be built from root\n",
    "- Not robust\n",
    "\n",
    "- Tree ensemble decide output label through **majority votes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311075f5",
   "metadata": {},
   "source": [
    "### 2. Sampling with Replacement\n",
    "- Building block for building tree ensemble\n",
    "- Datasets are slightly different with some having duplicates, and trees built using those datasets\n",
    "- So that one change wont affect whole model significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c177692",
   "metadata": {},
   "source": [
    "### 3. Random Forest Algorithm\n",
    "1. Given training set of size $m$:\n",
    "    * For $b = 1$ to $B$:\n",
    "        * Use sampling with replacement to create a new training set of size m\n",
    "        * Train a decision tree on the new dataset\n",
    "        * B has diminishing returns on larger values\n",
    "        \n",
    "    * Key idea: Randomizing the feature choice\n",
    "        * At each node, when choosing a feature to split, if $n$ features are available, pick a random subset of $k < n$ features and allow algorithm to only choose from that subset of features\n",
    "        * k = $\\sqrt{n}$\n",
    "        * Ensure higher chance of different splits of root node/ nodes near root by further randomizing\n",
    "        * Ensure trees vary more for accurate voting\n",
    "        * More robust as algorithms are trained more and averaging over small change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774958a",
   "metadata": {},
   "source": [
    "### 4. XGBoost (eXtreme Gradient Boosting)\n",
    "- Improved algorithm of decision tree ensembles\n",
    "<img  src=\"./images/Wk7_5.png\"  style=\" width:70%; padding: 10px 20px ; \">\n",
    "\n",
    "**Advantages:**\n",
    "1. Open source implementation of boosted trees\n",
    "2. Fast efficient implementation\n",
    "3. Good choice of default splitting criteria and criteria for when to stop splitting\n",
    "4. Built in regularisation to prevent overfitting\n",
    "5. Highly competitive algorithm for ML comps (Kaggle)\n",
    "\n",
    "#### Classification\n",
    "\n",
    "`from xgboost import XGBClassifier`  \n",
    "`model = XGBClassifier()`  \n",
    "`model.fit(X_train, y_train)`  \n",
    "`y_pred = model.predict(X_test)`\n",
    "\n",
    "\n",
    "#### Regression\n",
    "\n",
    "`from xgboost import XGBRegressor`  \n",
    "`model = XGBRegressor()`  \n",
    "`model.fit(X_train, y_train)`   \n",
    "`y_pred = model.predict(X_test)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bdc138",
   "metadata": {},
   "source": [
    "### 5. When to use decision trees/ neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2102e",
   "metadata": {},
   "source": [
    "<img  src=\"./images/Wk7_6.png\"  style=\" width:70%; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3512060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
