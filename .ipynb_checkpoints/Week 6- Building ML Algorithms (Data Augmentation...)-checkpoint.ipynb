{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472f1bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "from autils import * \n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# from assigment_utils import *\n",
    "\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134221d",
   "metadata": {},
   "source": [
    "## Advice on building ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e152c0",
   "metadata": {},
   "source": [
    "### 1. Debugging a learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fc0ca",
   "metadata": {},
   "source": [
    "What if it makes unacceptably large errors?\n",
    "For example, linear regression: \n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ \n",
    "- Fix high bias: Additional feature set, adding polynomial features, decrease $\\lambda$(regularisation parameter)?\n",
    "- Fix high variance: More training examples, smaller set of features, increasing $\\lambda$\n",
    "- Thus, we need machine learning diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3ef72",
   "metadata": {},
   "source": [
    "#### Machine learning diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84006181",
   "metadata": {},
   "source": [
    "Diagnostic: A test that you run to gain insight into what is/isn't working with a learning algorithm, to gain guidance\n",
    "into improving its performance.  \n",
    "\n",
    "\n",
    "Diagnostics can take time to implement but doing so can be a very good use of your time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576a443",
   "metadata": {},
   "source": [
    "### 2. Evaluating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889ef2d",
   "metadata": {},
   "source": [
    "Overfitting - fits the training data well but fail to generalise to new examples not in the training set **due to higher order polynomials/features**\n",
    "\n",
    "\n",
    "#### So how?\n",
    "* Split your original data set into Training set (70%) and Test set (30%). \n",
    "    * Use the training data to fit the parameters of the model\n",
    "    * Use the test data to evaluate the model on *new* data\n",
    "    * `#split the data using sklearn routine \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=1)`\n",
    "    \n",
    "* Develop an error function to evaluate your model.\n",
    "    * Compute test error and training error (without regularisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab9fea",
   "metadata": {},
   "source": [
    "### 3. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a70e7",
   "metadata": {},
   "source": [
    "Once parameters $\\vec{w},b$ are fit to the training set, training error $J_{train}(\\vec{w},b)$ is likely lower than the actual generalisation error. $J_{test}(\\vec{w},b)$ is better estimate of how well model will generalise to new data than $J_{train}(\\vec{w},b)$.\n",
    "\n",
    "- Choose nth-order polynomial which gives lowest generalisation error (not recommended)\n",
    "\n",
    "#### Solution: Split dataset into train, cross-validation and test set.\n",
    "- Training set (60%)\n",
    "- Cross-validation set (20%)\n",
    "- Test set (20%)\n",
    "- Cross validation set is used to test the nth order polynomial that is best suited, and gives a fair representation/estimate of the generalisation error when using test set\n",
    "- Better model / neural network selection procedure\n",
    "<img  src=\"./images/Wk6_1.png\"  style=\" width:600px; padding: 10px 20px ; \">\n",
    "\n",
    "\n",
    "| data             | % of total | Description |\n",
    "|------------------|:----------:|:---------|\n",
    "| training         | 60         | Data used to tune model parameters $w$ and $b$ in training or fitting |\n",
    "| cross-validation | 20         | Data used to tune other model parameters like degree of polynomial, regularization or the architecture of a neural network.|\n",
    "| test             | 20         | Data used to test the model after tuning to gauge performance on new data |\n",
    "\n",
    "\n",
    " **(DONT use test set to make decisions about model!)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882468d6",
   "metadata": {},
   "source": [
    "### 4. Bias and Variance (Diagnostic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c948a",
   "metadata": {},
   "source": [
    "<img  src=\"./images/Wk6_2.png\"  style=\" width:600px; padding: 10px 20px ; \">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5fc92",
   "metadata": {},
   "source": [
    "#### Regularisation and Bias/Variance\n",
    "- How to choose $\\lambda$ value?\n",
    "- Note: $\\lambda$ introduces a penalty term to the cost function, discouraging the model from fitting the training data too closely, by decreasing $w_j$\n",
    "- Variance is diff between training error and cross validation error\n",
    "<img  src=\"./images/Wk6_3.png\"  style=\" width:600px; padding: 10px 20px ; \">\n",
    "\n",
    "- Tune $\\lambda$ using $J_{cv}(\\vec{w},b)$\n",
    "\n",
    "<img  src=\"./images/Wk6_4.png\"  style=\" width:600px; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705e15d",
   "metadata": {},
   "source": [
    "#### Establishing a baseline level of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee7cea",
   "metadata": {},
   "source": [
    "- Compare bias/error in training error (10.8%) and cross validation error with \n",
    "    * **human level performance**(10.2%) = 0.6% net error\n",
    "    * **Competing algorithms performance**\n",
    "    * **Guess based on experience**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6abf51",
   "metadata": {},
   "source": [
    "#### Learning curves\n",
    "As training set size increases: $J_{cv}$ gradually decreases, $J_{train}$ gradually increases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabbf26",
   "metadata": {},
   "source": [
    "- High bias: Getting more training data will not help much, and will almost never outperform human level performance\n",
    "\n",
    "- High variance: Getting more training data likely to help as prevents overfitting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7632b",
   "metadata": {},
   "source": [
    "#### Neural networks and bias variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386adad7",
   "metadata": {},
   "source": [
    "- Large Neural Networks are low bias machines\n",
    "- Two main questions:\n",
    "    * Do well on training set? No- Bigger network\n",
    "    \n",
    "    * Do well on cross-validation set? No (High variance)- More data\n",
    "- Large NN usually do as well than smaller as long as regularisaion chose properly\n",
    "\n",
    "#### Regularised MNIST Model\n",
    "`layer_1 = Dense(units=25, activation='relu', kernel_regulariser=L2(0.01))`\n",
    "\n",
    "\n",
    "`layer_2 = ..., kernel_regulariser=L2(0.01)`\n",
    "\n",
    "\n",
    "`layer_3 = ..., kernel_regulariser=L2(0.01)`\n",
    "\n",
    "\n",
    "`model = Sequential([layer_1, layer_2, layer_3])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb18058",
   "metadata": {},
   "source": [
    "### 5. Iterative loop of ML Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e8352",
   "metadata": {},
   "source": [
    "Choose architecture(model, data, etc.) > Train Model > Diagnostics (bias, variance)\n",
    "- Require multiple iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96349290",
   "metadata": {},
   "source": [
    "#### Building a spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1bc1e",
   "metadata": {},
   "source": [
    "Supervised learning:\n",
    "- $\\vec{x}$ = features of email\n",
    "- $y$ = spam (1) or not spam (0)\n",
    "\n",
    "Features: list the top 10,000 words to compute $x_1,x_2,...,x_{10,000}$\n",
    "\n",
    "Ways to reduce error:\n",
    "- Collect more data. E.g, \"Honeypot\" project\n",
    "- Develop sophisticated features based on email routing in header\n",
    "- Define sophisticated fetaures from email body. Eg: \"discounting\"/\"discount\" same word?\n",
    "- Design algorithms to detect misspellings. Eg: \"w4tches, med1cine\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84209634",
   "metadata": {},
   "source": [
    "### 6. Error Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c15caad",
   "metadata": {},
   "source": [
    "$m_{cv}$ = 500 examples in cross validation set\n",
    "- Algorithm misclassifies 100\n",
    "- **Manually examine examples and categorise based on common traits**.\n",
    "\n",
    "Eg (spam classification):\n",
    "1. Pharma: 21\n",
    "2. Deliberate misspellings: 3\n",
    "3. Unusual email routing: 7\n",
    "4. Steal passwords (phishing): 18\n",
    "5. Spam message in embedded image: 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00a5d5",
   "metadata": {},
   "source": [
    "### 7. Adding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f9872",
   "metadata": {},
   "source": [
    "#### 7A. Data Augmentation\n",
    "Augmentation: modifying an existing training example to create a new training example.\n",
    "<img src=\"./images/Wk6_5.png\"  alt=\"Image 1\" style=\"width: 45%; display: inline-block; margin-right: 5px;\">\n",
    "<img src=\"./images/Wk6_6.png\"  alt=\"Image 2\" style=\"width: 45%; display: inline-block;\">\n",
    "\n",
    "- Can also apply data augmentation to speech recognition by adding noisy background, distortions,etc.\n",
    "- Modify/Distort in such a way that is similar to test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ee7ca",
   "metadata": {},
   "source": [
    "#### 7B. Data Synthesis\n",
    "Eg: Artificial data synthesis for photo Optimal Character Recognition (OCR)\n",
    "- Mostly for computer vision tasks\n",
    "<img  src=\"./images/Wk6_7.png\"  style=\" width:600px; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fa7b4",
   "metadata": {},
   "source": [
    "#### 7C. Data engineering\n",
    "- Taking a data-centric approach rather than model-centric approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a762bf9",
   "metadata": {},
   "source": [
    "### 8. Transfer learning\n",
    "- Using data from a different task of the **same input type**\n",
    "- By learning from pictures of cats, dogs, might have learn some plausible sets of parameters from earlier layers, and transferring parameters to new neural network, and starts parameters at pretty good place\n",
    "\n",
    "- Option 1: only train output layers parameters\n",
    "- Option 2: Train all parameters\n",
    "\n",
    "<img src=\"./images/Wk6_8.png\"  alt=\"Image 1\" style=\"width: 45%; display: inline-block; margin-right: 5px;\">\n",
    "<img src=\"./images/Wk6_9.png\"  alt=\"Image 2\" style=\"width: 45%; display: inline-block;\">\n",
    "\n",
    "Steps:\n",
    "1. Download neural network parameters pretrained on a large datset with same input data type as your application.\n",
    "\n",
    "2. Further train (fine tune) the network on your own data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bda5f",
   "metadata": {},
   "source": [
    "### 9. Full cycle of a Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b30599",
   "metadata": {},
   "source": [
    "<img  src=\"./images/Wk6_10.png\"  style=\" width:600px; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de3040",
   "metadata": {},
   "source": [
    "<img  src=\"./images/Wk6_11.png\"  style=\" width:600px; padding: 10px 20px ; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1df109",
   "metadata": {},
   "source": [
    "### 10. Skewed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac354175",
   "metadata": {},
   "source": [
    "#### Error metrics for skewed datasets\n",
    "\n",
    "Eg: Rare disease classification example\n",
    "- Might get 99% correct diagnoses, but 1% error on test set, but only 0.5% of patients have disease\n",
    "- But you can also get 99.5% accuracy/ 0.5% error by printing('0')!!\n",
    "\n",
    "#### Precision/Recall\n",
    "That way, if print 0, TP = 0; Precision = 0%\n",
    "\n",
    "Precision = $\\frac{TP}{TP+ FP}$  \n",
    "Recall = $\\frac{TP}{TP + FN}$ (also known as sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46896f39",
   "metadata": {},
   "source": [
    "#### Trading off precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229a718",
   "metadata": {},
   "source": [
    "If we raise threshold (very confident) for logistic regression (original: 0.5),  \n",
    "**Higher precision, lower recall**\n",
    "\n",
    "If reduce threshold (avoid missing),  \n",
    "**Lower precision, higher recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdd494",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "- To compare precision/recall\n",
    "- Choose highest\n",
    "\n",
    "Formula : $$\\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} = 2\\frac{PR}{P+R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac402462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
