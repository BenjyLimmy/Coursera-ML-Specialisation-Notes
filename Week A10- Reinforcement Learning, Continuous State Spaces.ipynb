{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab57125b",
   "metadata": {},
   "source": [
    "## 1. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84e99e",
   "metadata": {},
   "source": [
    "### 1A. What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be804b",
   "metadata": {},
   "source": [
    "Given state $s$, determine action $a$\n",
    "\n",
    "- We could use supervised learning, but extremely difficult as there are many states in real time and ambiguous.\n",
    "\n",
    "The key of reinforcement learning is learning through a **reward function**. \n",
    "\n",
    "Eg: Autonomous helicopter\n",
    "- Positive reward: Helicopter flying well (+1)\n",
    "- Negative reward: Helicopter flying poorly (-1000)\n",
    "\n",
    "- We cant program to pick \"good actions\" , but figured out by machine itself through rewards\n",
    "\n",
    "**Applications**:\n",
    "1. Controlling robots\n",
    "2. Factory optimisation\n",
    "3. Financial (stock) trading\n",
    "4. Playing games (including video games)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372532f",
   "metadata": {},
   "source": [
    "### 1B. Mars Rover Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28310fff",
   "metadata": {},
   "source": [
    "- State $s$\n",
    "- Action $a$\n",
    "- Reward from state $R(s)$\n",
    "- New state $s'$\n",
    "- At every time step - $(s, a, R(s), s')$\n",
    "- Terminal state- Program receives reward and terminates/ends function\n",
    "\n",
    "Eg: $(4, <-, 0, 3)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077775b",
   "metadata": {},
   "source": [
    "### 1C. Return in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9ae71",
   "metadata": {},
   "source": [
    "- Calculated by Discount Factor \n",
    "\n",
    "$$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... \\text{ until terminal state}$$\n",
    "$$\\text{Discount Factor }\\gamma = 0.9$$\n",
    "\n",
    "- Makes reinforcement learning algorithm impatient and credits more to faster rewards sooner and gives higher value\n",
    "- High $\\gamma$ = Very heavily discounts rewards in future\n",
    "- Similar concept to Time Value of money\n",
    "\n",
    "Eg:\n",
    "<img src= ./images/Wk10_1.png style=\"width:70%; padding:10px, 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac1769",
   "metadata": {},
   "source": [
    "### 1D. Policies in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc439c",
   "metadata": {},
   "source": [
    "- Policy $\\pi$\n",
    "- Function $\\pi(s) = a$, takes state and maps to an action, tells you what action $a$ to take in given state  $s$.\n",
    "- Eg:\n",
    "    * $\\pi(s) = a$\n",
    "    * $\\pi(2) = <-$\n",
    "    * $\\pi(3) = <-$\n",
    "    * $\\pi(4) = <-$\n",
    "    * $\\pi(5) = ->$\n",
    "    \n",
    "- **Goal** - Find policy $\\pi$ that maps each state to an action in which **RETURN** is **MAXIMIZED**.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d17219",
   "metadata": {},
   "source": [
    "<img src= ./images/Wk10_2.png style=\"width:70%; padding:10px, 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a748be",
   "metadata": {},
   "source": [
    "### 1E. Markov Decision Process\n",
    "- Key Idea - Future actions depend on current state, and not how machine got to the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd432c",
   "metadata": {},
   "source": [
    "<img src= ./images/Wk10_3.png style=\"width:70%; padding:10px, 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5a0aa",
   "metadata": {},
   "source": [
    "## 2. State-action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26ac5f",
   "metadata": {},
   "source": [
    "- Key quantity for developing a reinforcement learning algorithm\n",
    "- Denoted by $Q(s, a)$ = return if you\n",
    "    * start in state $s$\n",
    "    * take action $a$ (once)\n",
    "    * behave optimally after that (might be circular and hard to understand)\n",
    "    * <img src= ./images/Wk10_4.png style=\"width:70%; padding:10px, 20px;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95198e3e",
   "metadata": {},
   "source": [
    "<img src= ./images/Wk10_4.png style=\"width:70%; padding:10px, 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e477413",
   "metadata": {},
   "source": [
    "### 2A. Picking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4788395",
   "metadata": {},
   "source": [
    "- Choose action which has highest Q-value (duh)\n",
    "- Best possible **RETURN** from state $s$ is $\\max_{a}{Q(s,a)}$\n",
    "- Best action in state $s$ is action $a$ that gives $\\max_a {Q(s,a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b7284",
   "metadata": {},
   "source": [
    "### 2B. Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba743992",
   "metadata": {},
   "source": [
    "- **MOST IMPORTANT EQUATION** in Reinforcement Learning\n",
    "- Compute state-action value / $Q(s, a)$\n",
    "\n",
    "- $s$: current state\n",
    "- $R(s)$: reward of current state\n",
    "- $a$: current action\n",
    "- $s'$: state you get to after taking action $a$\n",
    "- $a'$: action you take in state $s'$\n",
    "\n",
    "$$Q(s,a) = R(s) + \\gamma \\max_{a'}Q(s',a')$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63251f2",
   "metadata": {},
   "source": [
    "<img src= ./images/Wk10_5.png style=\"width:70%; padding:10px, 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6adab",
   "metadata": {},
   "source": [
    "### 2C. Stochastic (Random) Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f385c3",
   "metadata": {},
   "source": [
    "- Include probability of accidentally doing the wrong action\n",
    "- Eg: going left instead of going right\n",
    "- In this case, we want to maximise the average value of the sum of discounted rewards (expected return), instead of return\n",
    "\n",
    "`misstep_prob = 0.4`\n",
    "\n",
    " \n",
    "$$\\text{Expected Return} = E[R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + ... ]$$\n",
    "\n",
    " \n",
    "$$Q(s,a) = R(s) + \\gamma E[\\max_{a'}Q(s',a')]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a42cad",
   "metadata": {},
   "source": [
    "## 3. Continuous State Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa93289",
   "metadata": {},
   "source": [
    "- Mars rover example with 6 different states represents **DISCRETE state spaces**\n",
    "- Continuous (Mars Rover):\n",
    "$$s = \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    \\theta \\text{ (0-360$^o$)}\\\\\n",
    "    \\dot{x} \\\\\n",
    "    \\dot{y} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Autonomous Helicopter**\n",
    "$$s = \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z \\\\\n",
    "    \\phi \\text{ (roll)}\\\\\n",
    "    \\theta \\text{ (pitch)}\\\\\n",
    "    \\omega \\text{ (yaw)}\\\\ \n",
    "    \\dot{x} \\\\\n",
    "    \\dot{y} \\\\\n",
    "    \\dot{z} \\\\\n",
    "    \\dot{\\phi} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\dot{\\omega} \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9cd2a",
   "metadata": {},
   "source": [
    "### 3A. Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e13cc0",
   "metadata": {},
   "source": [
    "$$s = \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    \\dot{x} \\\\\n",
    "    \\dot{y} \\\\\n",
    "    \\theta\\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    l (0,1)\\\\\n",
    "    r (0,1)\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$a\\text{ (actions)} = \\begin{bmatrix}\n",
    "    \\text{do nothing} \\\\\n",
    "    \\text{left thruster} \\\\\n",
    "    \\text{main thruster} \\\\\n",
    "    \\text{right thruster} \\\\\n",
    "\\end{bmatrix}$$\n",
    "<img src= ./images/Wk10_6.png style=\"width:70%; padding:10px, 20px;\">\n",
    "\n",
    "$$\\text{Policy}(\\pi) - \\text{pick action $a$ = $\\pi(s)$ so as to maximise return}$$\n",
    "$$\\lambda =  0.985$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa7c80",
   "metadata": {},
   "source": [
    "### B. Deep Reinforcement Learning (State- value function)\n",
    "- Train neural network to approximate state-value function $Q(s,a)$\n",
    "<img src= ./images/Wk10_7.png style=\"width:70%; padding:10px, 20px;\">\n",
    "\n",
    "How do you train neural network to output $Q(s,a)$?\n",
    "General Approach:\n",
    "* Use Bellman's Equation to create large training set ($x -> y$)\n",
    "* Use supervised learning to learn mapping from $x$(state-action pair) to $y$($Q(s,a)$)\n",
    "* Try many different possibilites to find results and rewards ($s,a,R(s),s'$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61821dd",
   "metadata": {},
   "source": [
    "#### Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201e68f",
   "metadata": {},
   "source": [
    "<img src= ./images/Wk10_8.png style=\"width:70%; padding:10px, 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be89bdf",
   "metadata": {},
   "source": [
    "#### Learning Algorithm\n",
    "1. Initialise neural network randomly as guess of Q(s,a)\n",
    "2. Repeat {\n",
    "    * Take actions in lunar lander. Get($s,a,R(s),s'$)\n",
    "    * Store 10,000 most recent ($s,a,R(s),s'$) tuples. (Replay Buffer)\n",
    "    * Train neural network:\n",
    "        * Create training set of 10,000 examples using\n",
    "        \n",
    "        $x = (s,a)$ and $y = R(s) + \\gamma max_{a'}Q(s',a')$\n",
    "        \n",
    "        * Train $Q_{new}$ such that $Q_{new}(s,a)\\approx y$\n",
    "    * Set $Q = Q_{new}$\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece26570",
   "metadata": {},
   "source": [
    "### 3C. Algorithm refinement (Improved neural network architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27444dda",
   "metadata": {},
   "source": [
    "<img src= ./images/Wk10_7.png style=\"width:50%; padding:10px, 20px; float: left;\">\n",
    "\n",
    "<img src= ./images/Wk10_9.png style=\"width:50%; padding:10px, 20px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984f654",
   "metadata": {},
   "source": [
    "Old: Need to carry out inference 4 different times for each action which is inefficient  \n",
    "New: Compute Q value for each action simultaneously, and efficiently calculates max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8e6db",
   "metadata": {},
   "source": [
    "### 3D. Algorithm refinement ($\\epsilon$-greedy policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277a60a",
   "metadata": {},
   "source": [
    "- Common way to take actions while still learning.\n",
    "\n",
    "- **Approach**: In some state $s$,\n",
    "\n",
    "    - Option 1 (not as good): Pick the action that maximizes $Q(s,a)$ (even if not great estimate yet). (Greedy, \"Exploitation\")\n",
    "    - Option 2: Pick the action that maximizes $Q(s,a)$ most of the time, but with a small probability (e.g. 5%) pick an action randomly. (In case, it initialises $Q(s,a)$ to wrong parameters (eg. never try good actions))\n",
    "    - Picking actions randomly is sometimes called an \"exploration step\".\n",
    "- $\\epsilon$ is the probability of taking an action randomly (e.g. 0.05).\n",
    "- **Trick**: Start with epsilon high and gradually decrease it over time, so that eventually you're taking greedy actions most of the time.\n",
    "- Epsilon-greedy exploration allows the agent to balance between exploring new actions and exploiting the actions that it currently believes to be the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d3d23",
   "metadata": {},
   "source": [
    "### 3E. Algorithm refinement (Mini-batch and Soft updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5278c",
   "metadata": {},
   "source": [
    "#### Mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dce767",
   "metadata": {},
   "source": [
    "- Mini batch gradient descent - refinement to the gradient descent algorithm that is used to **speed up both supervised and reinforcement learning** algorithms.\n",
    "- The idea is to **use a subset of the training examples**, called a mini batch, rather than the entire dataset on each iteration of the algorithm.\n",
    "- This **makes each iteration** of the algorithm **faster and more efficient**, particularly when dealing with large datasets.\n",
    "- In reinforcement learning, this could involve using a subset of the stored tuples in the replay buffer to train the neural network.\n",
    "<img src= ./images/Wk10_10.png style=\"width:49%; padding:10px, 100px; float: left;\">\n",
    "\n",
    "<img src= ./images/Wk10_11.png style=\"width:49%; padding:10px, 20px; \">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805190e9",
   "metadata": {},
   "source": [
    "#### Soft Updates\n",
    "Problem: Mini-batches might cause reinforcement learning algorithm to become worse/ have higher error when setting $Q = Q_{new}$\n",
    "\n",
    "- Soft updates is another refinement to the reinforcement learning algorithm that helps it converge to a good solution.\n",
    "$$w = 0.01w_{new} + 0.99w$$\n",
    "$$b = 0.01b_{new} + 0.99b$$\n",
    "- The idea is to update the target Q-network slowly, rather than copying the weights of the policy network to the target Q-network all at once.\n",
    "- This helps to stabilize the training process and prevent the algorithm from overfitting to/ diverge from the training data.\n",
    "- Soft updates are particularly useful in deep reinforcement learning, where overfitting is a common problem.\n",
    "- Soft update method helps to prevent $Q_{new}$ from becoming worse than the previous $Q$ as it makes a more gradual change to the neural network parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d6f11",
   "metadata": {},
   "source": [
    "### 4. State of Reinforcement Learning\n",
    "\n",
    "- Fewer applications than supervised and unsupervised learning.\n",
    "- Reinforcement learning is easier to get to work in simulated environments than in the real world, especially on real robots.\n",
    "- Odds of using supervised or unsupervised learning are higher than using reinforcement learning in practical applications.\n",
    "- Remains one of the major pillars of machine learning and has a lot of potential for future applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e6dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
